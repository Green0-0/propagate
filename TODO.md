# TODO, in order of priority
- Optimized LoRA backend with round robin adapter inference
- Automatic perturbation scale sweeping, rescale step size by sigma
- Write unit tests
- Colab/Kaggle notebooks
- Env training (ie. agentic, coding, multiturn etc), requires dataset input rework

- RLHF
- Sglang support
- Albatross support
- TPU support
- Proper model saving
- Trained demo models, benchmark suite
- Native quant format training

- Optimizer rework
- Random sampling rework (hadamard rachemacher)