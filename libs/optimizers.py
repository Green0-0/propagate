from abc import ABC, abstractmethod
from typing import List, Dict
from collections import OrderedDict

from libs.genome import Genome

import math

class Optimizer(ABC):
    def __init__(self, total_steps: int, learning_rate: float, seed_weight: float, warmup_steps: int = 0, scheduler: str = "none"):
        self.total_steps = total_steps
        self.learning_rate = learning_rate
        self.seed_weight = seed_weight
        self.warmup_steps = warmup_steps
        self.scheduler = scheduler


    @abstractmethod
    def get_step(self, genomes: List[Genome], current_step: int) -> Genome:
        """Returns a merged genome that represents the current optimization step."""
        pass

    def get_lr(self, current_step: int) -> float:
        if self.warmup_steps > 0 and current_step < self.warmup_steps:
            return self.learning_rate * (current_step / self.warmup_steps)

        t = max(0.0, min(1.0, (current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)))

        sched = (self.scheduler or "none").lower()
        if sched in ("none", "constant"):
            return self.learning_rate
        if sched.startswith("linear"):
            return self.learning_rate * (1.0 - t)
        if sched.startswith("cosine"):
            return self.learning_rate * 0.5 * (1.0 + math.cos(math.pi * t))
        raise ValueError(f"Unknown scheduler: {self.scheduler}")

class TestMaxOptimizer(Optimizer):
    def get_step(self, genomes: List[Genome], current_step: int) -> Genome:
        lr = self.get_lr(current_step)
        best_genome = max(genomes, key=lambda g: g.historical_rewards[-1])
        merged = Genome()
        merged.seeds = best_genome.seeds.copy()
        merged.seed_weights = [math.copysign(lr, w) if i >= best_genome.starting_index else w for i, w in enumerate(best_genome.seed_weights)]
        merged.historical_rewards = [float('-inf')] * len(merged.seeds)
        merged.starting_index = len(merged.seeds)
        return merged

class SimpleOptimizer(Optimizer):
    def get_step(self, genomes: List[Genome], current_step: int) -> Genome:
        """Perform a single gradient step on a list of genomes based on their rewards r and seeds s.
        The update rule for the new seeds follows the paper source [1/N * sum_j ((r_j - mean(r)) / stddev(r) * noise)]
        Since genomes may have taken different gradient steps in the past, the update rule for the previous gradients is simply an averaged merge.
        Mirrors are treated by taking the sign of the weight into account.
        The weights of these updates are summed together to form the weights of the new seeds. 
        
        Args:
            genomes (List[Genome]): The list of genomes to merge.

        Returns:
            Genome: The merged genome.
        """
        lr = self.get_lr(current_step)
        merged = Genome()
        reward_mean = sum([g.historical_rewards[-1] for g in genomes]) / len(genomes)
        reward_stddev = (sum([(g.historical_rewards[-1] - reward_mean) ** 2 for g in genomes]) / len(genomes)) ** 0.5
        new_seeds = {}
        old_seeds_count = {}
        for g in genomes:
            for i in range(len(g.seeds)):
                # If this is a new seed, apply the gradient step update. If this is an old seed (generated by a gradient step), retain its previous weight.
                seed = g.seeds[i]
                weight = g.seed_weights[i]
                if seed not in new_seeds:
                    if i < g.starting_index:
                        old_seeds_count[seed] = 1
                        new_seeds[seed] = weight
                    else:
                        new_seeds[seed] = math.copysign(1, weight) * lr * (1/len(genomes)) * (g.historical_rewards[-1] - reward_mean) / (reward_stddev + 1e-8)
                else:
                    if i < g.starting_index:
                        old_seeds_count[seed] += 1
                        new_seeds[seed] += weight
                    else:
                        new_seeds[seed] += math.copysign(1, weight) * lr * (1/len(genomes)) * (g.historical_rewards[-1] - reward_mean) / (reward_stddev + 1e-8)
        # Average the old seeds' weights
        for seed, count in old_seeds_count.items():
            new_seeds[seed] /= count

        # Update the merged genome
        for seed, weight in new_seeds.items():
            merged.seeds.append(seed)
            merged.seed_weights.append(weight)
            merged.historical_rewards.append(float('-inf'))  # placeholder sentinel value for previous rewards
        merged.starting_index = len(merged.seeds)
        return merged

class MomentumOptimizer(Optimizer):
    velocity_seeds: OrderedDict
    cutoff_seeds: int

    def __init__(self, total_steps: int, learning_rate: float, seed_weight: float, warmup_steps: int = 0, scheduler: str = "none", momentum: float = 0.9, cutoff_seeds = 2000):
        super().__init__(total_steps, learning_rate, seed_weight, warmup_steps, scheduler)
        self.momentum = momentum
        self.velocity_seeds = OrderedDict()
        # Having a cutoff keeps the size of the velocity array from growing unreasonably, which would create a lot of overhead during updates
        self.cutoff_seeds = cutoff_seeds

    def get_step(self, genomes: List[Genome], current_step: int) -> Genome:
        lr = self.get_lr(current_step)
        for seed in self.velocity_seeds:
            self.velocity_seeds[seed] *= self.momentum
       
        reward_mean = sum([g.historical_rewards[-1] for g in genomes]) / len(genomes)
        reward_stddev = (sum([(g.historical_rewards[-1] - reward_mean) ** 2 for g in genomes]) / len(genomes)) ** 0.5
        old_seeds = {}
        old_seeds_count = {}
        for g in genomes:
            for i in range(len(g.seeds)):
                # If this is a new seed (it's unlikely we have seen it before so we do not optimize for duplicates), add it to the velocity. If this is an old seed (generated by a gradient step), ignore it because we keep a running history of all old seeds.
                seed = g.seeds[i]
                weight = g.seed_weights[i]
                if i < g.starting_index:
                    if seed not in old_seeds:
                        old_seeds_count[seed] = 1
                        old_seeds[seed] = weight                        
                    else:
                        old_seeds_count[seed] += 1
                        old_seeds[seed] += weight
                else:
                    new_seed_value = math.copysign(1, weight) * lr * (1/len(genomes)) * (g.historical_rewards[-1] - reward_mean) / (reward_stddev + 1e-8)
                    if seed in self.velocity_seeds:
                        self.velocity_seeds[seed] += new_seed_value
                    else:
                        self.velocity_seeds[seed] = new_seed_value

        self.velocity_seeds = OrderedDict(list(self.velocity_seeds.items())[-self.cutoff_seeds:])

        # Average the old seeds' weights
        for seed, count in old_seeds_count.items():
            old_seeds[seed] /= count

        # Update the merged genome
        merged = Genome()
        for seed, weight in old_seeds.items():
            merged.seeds.append(seed)
            merged.seed_weights.append(weight)
            merged.historical_rewards.append(float('-inf'))  # placeholder sentinel value for previous rewards
        for seed, weight in self.velocity_seeds.items():
            merged.seeds.append(seed)
            merged.seed_weights.append(weight)
            merged.historical_rewards.append(float('-inf'))  # placeholder sentinel value for previous rewards
        merged.starting_index = len(merged.seeds)
        return merged